{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author/User:</b> <b style=\"color:#6666ff\">Sai Kumar Ramagiri</b><br>\n",
    "<b>Date of Submission:</b> <b style=\"color:#e69900\">20/08/2023</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h1 style=\"color:#006699; text-align:center\">MACHINE LEARNING</h1></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h2 style=\"color:#24478f; text-align:center\">ASSIGNMENT 9</h2></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "- Feature engineering is the process of crafting and refining input variables (features) from raw data to enhance the performance of machine learning models. It involves creating new features, selecting relevant ones, and transforming existing ones.\n",
    "- Key aspects include leveraging domain knowledge, handling missing values, scaling features, encoding categorical data, and addressing outliers. It's an iterative process that requires domain expertise and experimentation to optimize the model's ability to learn patterns and make accurate predictions from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b>\n",
    "\n",
    "**Feature selection** involves picking the most relevant features from data to enhance model performance and simplicity.\n",
    "- Its aim is to improve predictive accuracy, decrease overfitting, and reduce computation.\n",
    "- Methods include statistical tests (Univariate), model-based ranking (RFE, Feature Importance), and regularization (L1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "**Filter Approach:**\n",
    "In the filter approach, features are selected based on statistical measures like correlation or mutual information with the target variable. It's a preprocessing step independent of the model. Pros include speed and simplicity, but it might overlook interactions. Cons involve not considering the model's performance.\n",
    "\n",
    "**Wrapper Approach:**\n",
    "The wrapper approach evaluates feature subsets using a specific machine learning algorithm and selects the one with the best performance through cross-validation. It's model-dependent and can capture interactions, but it's computationally expensive. Pros include tailored feature selection, but cons involve high computational cost and potential overfitting to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b>\n",
    "\n",
    "**i. Feature Selection Process:**\n",
    "Feature selection involves choosing relevant input variables to enhance model performance. It comprises relevance assessment, redundancy evaluation, model-based ranking, and iterative validation to retain informative features while reducing noise.\n",
    "\n",
    "**ii. Key Principle of Feature Extraction:**\n",
    "Feature extraction involves transforming raw data into a compact and meaningful representation. For instance, in image recognition, extracting edges and textures from images instead of using raw pixel values. Common algorithms include Principal Component Analysis (PCA) for dimensionality reduction and t-SNE for visualizing high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">5. Describe the feature engineering process in the sense of a text categorization issue.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "In text categorization, the feature engineering process involves converting raw text data into numerical features that machine learning models can understand. It includes steps like text tokenization, applying techniques like TF-IDF or word embeddings, handling stopwords, creating n-grams, and selecting informative features. This process transforms text into a format that captures semantic meaning and patterns, enabling accurate classification by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "**Cosine similarity** is well-suited for text categorization due to its ability to measure the similarity between documents regardless of their lengths. It focuses on the direction of vectors, making it effective for capturing semantic meaning in text. \n",
    "\n",
    "Given the document-term matrix rows:\n",
    "- Vector A: (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
    "- Vector B: (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
    "\n",
    "The cosine similarity between the two vectors is approximately 0.675, indicating a moderate level of resemblance in the high-dimensional space of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "**i. Hamming Distance:**\n",
    "i. The Hamming distance formula calculates the differing bits between binary strings of equal length.\n",
    "   - Hamming Distance = Number of differing bits\n",
    "   - Between 10001011 and 11001111: Hamming Distance = 2\n",
    "\n",
    "**ii. Jaccard Index and Similarity Matching Coefficient:**\n",
    "ii. For given feature vectors:\n",
    "   - Jaccard Index (A, B) ≈ 0.667; Jaccard Index (A, C) ≈ 0.333\n",
    "   - Similarity Matching Coefficient (A, B) ≈ 0.667; Similarity Matching Coefficient (A, C) = 0.25\n",
    "   - Features A and B are more similar in both metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "**High-Dimensional Data Set:**\n",
    "A \"high-dimensional data set\" has many features (dimensions) relative to the number of samples. Each data point is characterized by numerous attributes.\n",
    "\n",
    "**Examples:**\n",
    "1. **Genomics:** DNA sequences with thousands of genetic markers.\n",
    "2. **Text Analytics:** Documents represented by many words in a large vocabulary.\n",
    "3. **Image Processing:** Images described by pixel values in multiple dimensions.\n",
    "\n",
    "**Difficulties and Solutions:**\n",
    "Using machine learning on high-dimensional data leads to overfitting, increased computational demands, and noise. To address this, consider feature selection, dimensionality reduction (PCA, t-SNE), regularization, domain knowledge-guided selection, and ensemble techniques to improve model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "Certainly, here are the quick notes:\n",
    "\n",
    "1. **PCA (Principal Component Analysis):**\n",
    "   - PCA stands for Principal Component Analysis, a dimensionality reduction method used in statistics and machine learning.\n",
    "   - It transforms data to capture major variations in a reduced set of dimensions.\n",
    "\n",
    "2. **Use of Vectors:**\n",
    "   - Vectors are quantities with magnitude and direction, essential in representing data points, features, and model parameters.\n",
    "   - Machine learning leverages vectors for efficient computations and to capture relationships in multi-dimensional space.\n",
    "\n",
    "3. **Embedded Technique:**\n",
    "   - An embedded technique incorporates feature selection within model training.\n",
    "   - Algorithms like decision trees or LASSO perform feature selection while learning, improving model performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "1. **Sequential Backward Exclusion vs. Sequential Forward Selection:**\n",
    "   - Sequential Backward Exclusion removes features step-by-step from an existing set, while Sequential Forward Selection adds features incrementally based on model performance.\n",
    "   \n",
    "2. **Function Selection Methods - Filter vs. Wrapper:**\n",
    "   - Filter methods use statistical metrics to evaluate features' relevance independently, while wrapper methods employ a specific model's performance to assess feature subsets.\n",
    "\n",
    "3. **SMC vs. Jaccard Coefficient:**\n",
    "   - Similarity Matching Coefficient (SMC) measures shared elements between binary vectors, while Jaccard coefficient quantifies shared elements' proportion in set-based data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
