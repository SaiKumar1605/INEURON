{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author/User:</b> <b style=\"color:#6666ff\">Sai Kumar Ramagiri</b><br>\n",
    "<b>Date of Submission:</b> <b style=\"color:#e69900\">20/08/2023</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h1 style=\"color:#006699; text-align:center\">MACHINE LEARNING</h1></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h2 style=\"color:#24478f; text-align:center\">ASSIGNMENT 8</h2></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">1. What exactly is a feature? Give an example to illustrate your point.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "- A feature in machine learning is a measurable attribute or characteristic of an entity that is used as input for a model.\n",
    "- For instance, in a spam email detection model, features could include the frequency of certain words, presence of specific patterns, and sender's domain.\n",
    "- These features provide data that the model analyzes to differentiate between spam and non-spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">2. What are the various circumstances in which feature construction is required?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b>\n",
    "\n",
    "Feature construction is needed when:\n",
    "1. **High Dimensionality:** To reduce the number of features and avoid overfitting.\n",
    "2. **Improved Representation:** To capture more meaningful patterns or relationships in the data for better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">3. Describe how nominal variables are encoded.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "Nominal variables are categorical variables with unordered and distinct categories. They are encoded using techniques like \"one-hot encoding,\" where each category becomes a binary feature (0 or 1), representing its presence or absence. This prevents the model from assuming any inherent order among categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">4. Describe how numeric features are converted to categorical features.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "Numeric features can be converted to categorical features by \"binning\" or \"discretization.\" This involves dividing the numeric range into intervals or bins and assigning a categorical label to each interval. For instance, age values (numeric) can be converted to age groups (categorical) like \"young,\" \"middle-aged,\" and \"senior.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this\n",
    "approach?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "**Feature Selection Wrapper Approach:**\n",
    "The wrapper approach for feature selection involves using a machine learning model's performance as a criterion for selecting features. It evaluates subsets of features by training and testing the model iteratively, aiming to find the subset that optimizes model performance.\n",
    "\n",
    "**Advantages:**\n",
    "- Considers interactions between features.\n",
    "- Customizable for specific model goals.\n",
    "- May lead to better predictive accuracy.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Computationally intensive for large feature sets.\n",
    "- Prone to overfitting due to evaluating subsets on the same data.\n",
    "- May not handle multicollinearity well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">6. When is a feature considered irrelevant? What can be said to quantify it?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "A feature is considered irrelevant when it doesn't significantly affect the model's predictions or classifications. It can be quantified by observing its low correlation with the target variable or by measuring its low feature importance score in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">7. When is a function considered redundant? What criteria are used to identify features that could\n",
    "be redundant?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "- A function is considered redundant when its output can be derived or closely approximated by another function within the same context.\n",
    "- Features that could be redundant are identified through correlation analysis, where high correlation coefficients indicate similar information content between features. Additionally, using techniques like variance inflation factor (VIF) helps detect multicollinearity, suggesting redundant features with high interdependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">8. What are the various distance measurements used to determine feature similarity?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "Various distance measurements for feature similarity include Euclidean distance, cosine similarity, Jaccard similarity, Hamming distance, and Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">9. State difference between Euclidean and Manhattan distances?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "The main difference between Euclidean and Manhattan distances is that Euclidean distance measures the shortest straight-line distance between two points in a multi-dimensional space, considering both horizontal and vertical components, while Manhattan distance (also known as City Block distance) measures the sum of the absolute differences between corresponding coordinates of two points, considering only horizontal and vertical movements along axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">10. Distinguish between feature transformation and feature selection.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "**Feature Transformation:** Involves altering feature values or representations to improve data quality, such as normalization, scaling, and dimensionality reduction.\n",
    "\n",
    "**Feature Selection:** Focuses on choosing a subset of the most relevant features from the original set to enhance model efficiency and reduce overfitting, involving methods like filtering, wrapper approaches, and embedded techniques.\n",
    "\n",
    "In summary, feature transformation modifies feature values, while feature selection narrows down the set of features used for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">11. Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "1. **SVD (Singular Value Decomposition):** SVD is a matrix factorization technique used for dimensionality reduction and noise reduction, decomposing a matrix into singular values and corresponding matrices for efficient data representation.\n",
    "\n",
    "2. **Collection of Features Using a Hybrid Approach:** A hybrid approach combines different feature selection or extraction methods, enhancing model performance by leveraging strengths from various techniques for better feature quality and model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
