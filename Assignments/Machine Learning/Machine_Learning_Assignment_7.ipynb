{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author/User:</b> <b style=\"color:#6666ff\">Sai Kumar Ramagiri</b><br>\n",
    "<b>Date of Submission:</b> <b style=\"color:#e69900\">20/08/2023</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h1 style=\"color:#006699; text-align:center\">MACHINE LEARNING</h1></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h2 style=\"color:#24478f; text-align:center\">ASSIGNMENT 7</h2></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function&#39;s fitness assessed?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "- The target function in machine learning is the ideal mapping between input variables and their corresponding output or prediction.\n",
    "- For instance, in a house price prediction scenario, the target function maps house features (e.g., size, location) to their respective prices.\n",
    "- A target function's fitness is assessed by measuring the accuracy or performance of a predictive model in approximating the true relationship between inputs and outputs using evaluation metrics like mean squared error or R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">2. What are predictive models, and how do they work? What are descriptive types, and how do you\n",
    "use them? Examples of both types of models should be provided. Distinguish between these two forms of models.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b>\n",
    "\n",
    "**Predictive Models:**\n",
    "Predictive models learn patterns from historical data to make predictions about future or unseen data. They work by identifying relationships between input features and target outputs, enabling them to make informed predictions. Examples include regression models (predicting numerical values) and classification models (predicting categories).\n",
    "\n",
    "**Descriptive Models:**\n",
    "Descriptive models aim to summarize and explain data patterns, helping us understand relationships within the data. They work by visualizing and interpreting data, providing insights without necessarily making predictions. Examples include clustering algorithms (grouping similar data points) and dimensionality reduction techniques (simplifying complex data while preserving important information).\n",
    "\n",
    "**Distinguishing:**\n",
    "Predictive models focus on making accurate predictions, while descriptive models emphasize understanding data patterns. Predictive models require labeled data for training, whereas descriptive models can work with unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various measurement parameters.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "Assessing a classification model's efficiency involves:\n",
    "1. **Confusion Matrix:** True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN).\n",
    "\n",
    "Measurement Parameters:\n",
    "1. **Accuracy:** (TP + TN) / Total instances.\n",
    "2. **Precision:** TP / (TP + FP).\n",
    "3. **Recall (Sensitivity):** TP / (TP + FN).\n",
    "4. **Specificity (True Negative Rate):** TN / (TN + FP).\n",
    "5. **F1-Score:** 2 * (Precision * Recall) / (Precision + Recall).\n",
    "6. **ROC Curve:** True Positive Rate vs. False Positive Rate.\n",
    "7. **AUC-ROC:** Area Under the ROC Curve.\n",
    "8. **PR Curve:** Precision vs. Recall.\n",
    "9. **AUC-PR:** Area Under the PR Curve.\n",
    "10. **F-beta Score:** Adjusted F1-score with parameter beta.\n",
    "11. **Matthews Correlation Coefficient (MCC):** Balanced measure for all confusion matrix values.\n",
    "12. **Cohen's Kappa:** Agreement between predicted and actual classifications.\n",
    "13. **Class-wise Metrics:** Metrics per class for multi-class problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">4.\n",
    "i. In the sense of machine learning models, what is underfitting? What is the most common\n",
    "reason for underfitting?\n",
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "iii. In the sense of model fitting, explain the bias-variance trade-off.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "i. **Underfitting:** Underfitting is when a model is too simple to capture data patterns, leading to poor performance; common due to overly basic models.\n",
    "\n",
    "ii. **Overfitting:** Overfitting is when a model is too complex, fitting training data too closely and generalizing poorly; occurs with excessive model complexity or noisy data.\n",
    "\n",
    "iii. **Bias-Variance Trade-off:** The bias-variance trade-off balances model simplicity (bias) and complexity (variance) to achieve optimal generalization; moderate complexity minimizes both errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "Yes, you can boost a learning model's efficiency by improving data quality, feature selection, hyperparameter tuning, using ensemble methods, and leveraging advanced architectures, leading to better predictions and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">6. How would you rate an unsupervised learning model&#39;s success? What are the most common\n",
    "success indicators for an unsupervised learning model?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "- An unsupervised learning model's success is typically rated based on its ability to discover meaningful patterns, structures, or groupings within the data.\n",
    "- Common success indicators include silhouette score, Davies-Bouldin index, and visual assessments like cluster separation and cohesion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "No, it's not advisable. Classification models are designed for discrete class labels, making them unsuitable for continuous numerical data, and regression models are tailored for continuous numerical predictions, making them inappropriate for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">8. Describe the predictive modeling method for numerical values. What distinguishes it from\n",
    "categorical predictive modeling?</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "Predictive modeling for numerical values uses regression algorithms to forecast continuous outcomes, while categorical predictive modeling employs classification algorithms to predict discrete class labels, distinguishing by data type and algorithm choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">9. The following data were collected when using a classification model to predict the malignancy of a\n",
    "group of patients&#39; tumors:\n",
    "i. Accurate estimates – 15 cancerous, 75 benign\n",
    "ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure.</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "Given the following data:\n",
    "\n",
    "i. Accurate estimates –\n",
    "   - True Positives (TP) = 15 (cancerous)\n",
    "   - True Negatives (TN) = 75 (benign)\n",
    "\n",
    "ii. Wrong predictions –\n",
    "   - False Positives (FP) = 7 (benign, predicted as cancerous)\n",
    "   - False Negatives (FN) = 3 (cancerous, predicted as benign)\n",
    "\n",
    "Total instances = TP + TN + FP + FN = 15 + 75 + 7 + 3 = 100\n",
    "\n",
    "1. **Error Rate:** (FP + FN) / Total = (7 + 3) / 100 = 0.1 (10%)\n",
    "\n",
    "2. **Kappa Value:**\n",
    "   - Calculate observed agreement (Observed Agreement = (TP + TN) / Total = 0.9)\n",
    "   - Calculate expected agreement by chance ((TP + FP) * (TP + FN) + (TN + FP) * (TN + FN)) / (Total^2) = 0.505\n",
    "   - Kappa Value = (Observed Agreement - Expected Agreement) / (1 - Expected Agreement) = (0.9 - 0.505) / (1 - 0.505) = 0.792\n",
    "\n",
    "3. **Sensitivity (True Positive Rate):** TP / (TP + FN) = 15 / (15 + 3) = 0.833\n",
    "\n",
    "4. **Precision:** TP / (TP + FP) = 15 / (15 + 7) = 0.682\n",
    "\n",
    "5. **F-measure:** 2 * (Precision * Sensitivity) / (Precision + Sensitivity) = 2 * (0.682 * 0.833) / (0.682 + 0.833) = 0.751\n",
    "\n",
    "So, for the given classification model:\n",
    "- Error Rate: 10%\n",
    "- Kappa Value: 0.792\n",
    "- Sensitivity: 0.833\n",
    "- Precision: 0.682\n",
    "- F-measure: 0.751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">10. Make quick notes on:\n",
    "1. The process of holding out\n",
    "2. Cross-validation by tenfold\n",
    "3. Adjusting the parameters</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "1. **Holding Out:** Splitting data into training and validation sets, reserving one portion for training and the other for evaluating model performance.\n",
    "   \n",
    "2. **Tenfold Cross-Validation:** Dividing data into ten subsets, iteratively using nine for training and one for validation, helping assess model generalization.\n",
    "\n",
    "3. **Adjusting Parameters:** Tweaking model hyperparameters to optimize performance, often using techniques like grid search to find the best configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 style=\" color: #ff3300\">11. Define the following terms:\n",
    "1. Purity vs. Silhouette width\n",
    "2. Boosting vs. Bagging\n",
    "3. The eager learner vs. the lazy learner</h4>\n",
    "\n",
    "<b style =\"color: #00b386\">Answer:</b> \n",
    "\n",
    "1. **Purity vs. Silhouette Width:**\n",
    "   - Purity measures cluster homogeneity, assessing how similar data points are within clusters. Silhouette width evaluates cluster separation and cohesion, indicating how well-separated clusters are.\n",
    "\n",
    "2. **Boosting vs. Bagging:**\n",
    "   - Boosting combines weak models sequentially, focusing on misclassified instances to improve overall model accuracy. Bagging employs parallel weak models and aggregates their predictions for better generalization.\n",
    "\n",
    "3. **Eager Learner vs. Lazy Learner:**\n",
    "   - Eager learners build a model during training and generalize using pre-built structures. Lazy learners delay model creation until prediction, adapting directly to training instances for greater flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
